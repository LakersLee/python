
	==================爬虫==================

一、爬虫注意事项
	
	@证书的问题——导入
	import ssl
	ssl._create_default_https_context = ssl._create_unverified_context

	1.合法性：robots文件，用于限制爬虫访问站点的链接(https://www.jd.com/robots.txt)
		User-agent: * 					#所有的用户代理
		Disallow: /?* 					#禁止爬取主页下所有网站
		Disallow: /pop/*.html 			
		Disallow: /pinpai/*.html?* 
		User-agent: EtaoSpider 			#代理为EtaoSpider。
		Disallow: / 					#EtaoSpider不能爬主页，其他的都可以，
										可能是自我监测第三方公司（以下同理）
		User-agent: HuihuiSpider 
		Disallow: / 
		User-agent: GwdangSpider 
		Disallow: / 
		User-agent: WochachaSpider 
		Disallow: /

	2.用户代理：一个爬虫的身份，默认的python写好的爬虫代码事python3.6-spider
		· 目标服务器可以知道你是谁
		· 自己的爬虫名字叫成：EtaoSpider
		· EtaoSpider：http协议，User-agent->request字段中的一个属性
	3.网站地图：sitemap.xml，查看当前网站（站点）下哪些内容在长期更新


二、下载网页
	
	1.浏览器打开变成代码代开：获取（CSS\JS)代码
		· 1——pachong.py，代码如下：
		· from urllib.request import urlopen
			· res = urlopen（url）：根据链接打开，返回一个Response
				<http.client.HTTPResponse object at 0x10311f320>
			·res.read().decode()
				对读取的res进行读取，因为为二进制流数据，需要decode解码（默认解码方式utf-8），获取网页源码
			· res.info()
				远程服务器返回的相应头信息
			· res.getcode()
				获取状态码 正常：200  3xx：重定向   其他异常：无结果
				错误链接：https://httpstat.us/500
			·res.geturl()
				返回当前打开的url站点（用于自动化爬虫，知道在哪里爬东西，不会迷路）

	2.访问站点报错（2_pachong.py)

		· 5xx：可挽救错误，尝试重新访问
		· 4xx：不可挽救错误，不能让程序报错，终止爬虫
		· 报错的返回值

*			***<<<  URLError是HttpError的父类，要从小的捕获  >>>***

			1.服务可达，但是访问被拒绝(HttpError)——————可挽救
				· 403:服务器拒绝（拉黑了）
				· 404:没有该内容
				· 500:服务器内部错误（url = 'https://httpstat.us/500'）
					urllib.error.HTTPError: HTTP Error 500: Internal Server Error
				· 503:套接字满了，服务器卡了
				· 505:http版本不支持，古董服务器
			2.服务不可达（URLError)——————不可挽救
				· 不存在链接(url = 'https://www.xjioahsuabsuuag.com')
					urllib.error.URLError: <urlopen error [Errno 8] nodename nor servname provided, or not known>
				· 被墙了（url = 'https://www.youtube.com'）
					urllib.error.URLError: <urlopen error [Errno 65] No route to host>
				· 没有网络

三、应用函数

	1.hasattr(e,'code')
		· 从错误e中获取code节点的内容e.code类型为int，用来判断报错的类型











